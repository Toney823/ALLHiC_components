#!/usr/bin/env python
import argparse
from genericpath import exists, getctime
import os
import re
from sys import path
import pysam
import time


def time_print(info, type='info'):
    if type != 'info':
        info = "\033[35m%s\033[0m"%info
    print("\033[32m%s\033[0m %s"%(time.strftime('[%H:%M:%S]',time.localtime(time.time())), info))


def get_opts():
    group = argparse.ArgumentParser()
    group.add_argument('-r', '--ref', help="Contig level assembly fasta", required=True)
    group.add_argument('-b', '--bam', help="Unprunned bam", required=True)
    group.add_argument('-c', '--cluster', help="Cluster file of contigs", required=True)
    group.add_argument('-n', '--counts', help="count REs file", required=True)
    group.add_argument('-g', '--gff3', help="Gff3 file generated by gmap cds to contigs", required=True)
    group.add_argument('-j', '--jcvi', help="CDS file for jcvi, bed file with same prefix must exist in the same position", required=True)
    group.add_argument('-e', '--exclude', help="cluster which need no rescue, default=\"\", split by comma", default="")
    group.add_argument('-w', '--workdir', help="Work directory, default=wrkdir", default="wrkdir")
    return group.parse_args()


def read_fasta(in_fa):
    fa_db = {}
    with open(in_fa, 'r') as fin:
        for line in fin:
            if line[0] == '>':
                id = line.strip().split()[0][1:]
                fa_db[id] = []
            else:
                fa_db[id].append(line.strip())
    for id in fa_db:
        fa_db[id] = ''.join(fa_db[id])
    
    return fa_db


def create_qry_file(source_cds, gff, target_cds, target_bed):
    src_cds_db = read_fasta(source_cds)
    idx = 1
    qry_db = {}
    with open(target_cds, 'w') as fcds:
        with open(target_bed, 'w') as fbed:
            with open(gff, 'r') as fin:
                for line in fin:
                    if line.strip() == '' or line[0] == '#':
                        continue
                    data = line.strip().split()
                    if data[2] != 'gene':
                        continue
                    id = re.findall(r'Name=(.*)', data[8])[0].split(';')[0]
                    new_id = "%s_%d"%(id, idx)
                    idx += 1
                    chrn = data[0]
                    sp = int(data[3])
                    ep = int(data[4])
                    if ep <= sp:
                        continue
                    direct = data[6]

                    if chrn not in qry_db:
                        qry_db[chrn] = set()
                    qry_db[chrn].add(new_id)
                    fcds.write(">%s\n%s\n"%(new_id, src_cds_db[id]))
                    fbed.write("%s\t%d\t%d\t%s\t0\t%s\n"%(chrn, sp, ep, new_id, direct))
    return qry_db


def read_anchors(anchors_file):
    anchor_db = {}
    with open(anchors_file, 'r') as fin:
        for line in fin:
            if line.strip() == '' or line[0] == '#':
                continue
            data = line.strip().split()
            qry_gn = data[0]
            ref_gn = data[1]
            anchor_db[qry_gn] = ref_gn
    return anchor_db


def convert_query_db(qry_db, anchor_db):
    new_qry_db = {}
    for chrn in qry_db:
        new_qry_db[chrn] = set()
        for gn in qry_db[chrn]:
            if gn in anchor_db:
                new_qry_db[chrn].add(anchor_db[gn])
    return new_qry_db


def get_ovlp(qry_set1, qry_set2):
    return len(qry_set1.intersection(qry_set2))


def get_clusters(clu):
    clu_db = {}
    clu_ctgs = {}
    with open(clu, 'r') as fin:
        for line in fin:
            if line[0] == '#':
                continue
            data = line.strip().split()
            chrn = data[0]
            ctgs = data[2:]
            clu_db[chrn] = ctgs
            for ctg in ctgs:
                clu_ctgs[ctg] = chrn
    return clu_db, clu_ctgs


def get_hic_signal(bam):
    signals = {}
    with pysam.AlignmentFile(bam, 'rb') as fin:
        for line in fin:
            ctg1 = line.reference_name
            pos1 = line.reference_start
            ctg2 = line.next_reference_name
            pos2 = line.next_reference_start
            if pos1==-1 or pos2==-1:
                continue
            if ctg1 not in signals:
                signals[ctg1] = {}
            if ctg2 not in signals[ctg1]:
                signals[ctg1][ctg2] = 0
            signals[ctg1][ctg2] += 1

            if ctg2 not in signals:
                signals[ctg2] = {}
            if ctg1 not in signals[ctg2]:
                signals[ctg2][ctg1] = 0
            signals[ctg2][ctg1] += 1
    return signals


def get_counts(counts):
    header = ""
    counts_db = {}
    with open(counts, 'r') as fin:
        for line in fin:
            if line[0] == '#':
                header = line
            else:
                ctg = line.strip().split()[0]
                counts_db[ctg] = line
    
    return header, counts_db


def ALLHiC_rescue(ref, bam, clu, counts, gff3, jprex, exclude, wrk):
    if not os.path.exists(wrk):
        os.mkdir(wrk)
    
    ref = os.path.abspath(ref)
    bam = os.path.abspath(bam)
    clu = os.path.abspath(clu)
    counts = os.path.abspath(counts)
    bed = os.path.abspath(jprex+'.bed')
    cds = os.path.abspath(jprex+'.cds')
    gff3 = os.path.abspath(gff3)

    exclude_set = set()
    if exclude != "":
        for grp in exclude.split(','):
            exclude_set.add(grp)

    jprex = jprex.split('/')[-1]
    time_print("Entering: %s"%wrk)
    os.chdir(wrk)

    os.system("ln -sf %s %s.cds"%(cds, jprex))
    os.system("ln -sf %s %s.bed"%(bed, jprex))
    new_cds = "dup.cds"
    new_bed = "dup.bed"

    qry_db = create_qry_file(cds, gff3, new_cds, new_bed)
    
    if not os.path.exists("dup.%s.anchors"%jprex):
        time_print("Running jcvi", type="important")
        cmd = "python -m jcvi.compara.catalog ortholog dup %s &> jcvi.log"%jprex
        os.system(cmd)
    else:
        time_print("Anchors file found, skip", type="important")
    
    time_print("Loading anchors file")
    anchor_db = read_anchors("dup.%s.anchors"%jprex)
    
    time_print("Converting query db")
    qry_db = convert_query_db(qry_db, anchor_db)

    time_print("Loading clusters")
    clu_db, clu_ctgs = get_clusters(clu)
    clu_set = {}
    for chrn in clu_db:
        clu_set[chrn] = set()
        for ctg in clu_db[chrn]:
            if ctg not in qry_db:
                continue
            clu_set[chrn] = clu_set[chrn].union(qry_db[ctg])
    
    remain_ctgs = []
    ctg_db = read_fasta(ref)

    for ctg in ctg_db:
        if ctg not in clu_ctgs:
            remain_ctgs.append([ctg, len(ctg_db[ctg])])
    
    time_print("Loading HiC signals")
    signal_db = get_hic_signal(bam)

    time_print("Get best matches")
    for ctg, ctgl in sorted(remain_ctgs, key=lambda x: x[1], reverse=True):
        score_list = []
        if ctg not in signal_db:
            continue
        for ctg2 in signal_db[ctg]:
            if ctg2 not in clu_ctgs:
                continue
            sig = signal_db[ctg][ctg2]
            ovlp = 0
            chrn = clu_ctgs[ctg2]
            if ctg in qry_db:
                ovlp = get_ovlp(qry_db[ctg], clu_set[chrn])
            score_list.append([ovlp, sig, chrn])
        if len(score_list)==0:
            continue
        for best_match in sorted(score_list, key=lambda x: [x[0], -x[1]]):
            if best_match[2] in exclude_set:
                continue
            else:
                break
        
        if best_match[1] < 10:
            continue
        time_print("\t%s matched %s, sig: %d, ovlp: %d"%(ctg, best_match[2], best_match[1], best_match[0]))
        clu_db[best_match[2]].append(ctg)
        if ctg in qry_db:
            clu_set[best_match[2]] = clu_set[best_match[2]].union(qry_db[ctg])
    
    time_print("Writing new groups")
    header, counts_db = get_counts(counts)
    for chrn in clu_db:
        with open("%s.txt"%chrn, 'w') as fout:
            fout.write(header)
            for ctg in clu_db[chrn]:
                fout.write(counts_db[ctg])     
        
    os.chdir("..")
    time_print("Finished")
    

if __name__ == "__main__":
    opts = get_opts()
    ref = opts.ref
    bam = opts.bam
    clu = opts.cluster
    counts = opts.counts
    gff3 = opts.gff3
    jprex = opts.jcvi
    jprex = '.'.join(jprex.split('.')[:-1])
    exclude = opts.exclude
    wrk = opts.workdir
    ALLHiC_rescue(ref, bam, clu, counts, gff3, jprex, exclude, wrk)
